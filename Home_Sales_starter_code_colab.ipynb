{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_KW73O2e3dw",
    "outputId": "d48a498e-e30d-43e5-d85b-3081b50ff001"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open '$SPARK_VERSION-bin-hadoop3.tgz'\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j in /content/spark-3.4.0-bin-hadoop3\\python, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3_1\\Lib\\site-packages\\findspark.py:159\u001b[0m, in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     py4j \u001b[38;5;241m=\u001b[39m glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(spark_python, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlib\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpy4j-*.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Start a SparkSession\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m findspark\u001b[38;5;241m.\u001b[39minit()\n",
      "File \u001b[1;32m~\\anaconda3_1\\Lib\\site-packages\\findspark.py:161\u001b[0m, in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    159\u001b[0m         py4j \u001b[38;5;241m=\u001b[39m glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(spark_python, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlib\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpy4j-*.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find py4j in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, your SPARK_HOME may not be configured correctly\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    163\u001b[0m                 spark_python\n\u001b[0;32m    164\u001b[0m             )\n\u001b[0;32m    165\u001b[0m         )\n\u001b[0;32m    166\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath[:\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m sys_path \u001b[38;5;241m=\u001b[39m [spark_python, py4j]\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# already imported, no need to patch sys.path\u001b[39;00m\n",
      "\u001b[1;31mException\u001b[0m: Unable to find py4j in /content/spark-3.4.0-bin-hadoop3\\python, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Find the latest version of spark 3.x  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.4.0'\n",
    "spark_version = 'spark-3.4.0'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XbWNf1Te5fM"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wOJqxG_RPSwp"
   },
   "outputs": [],
   "source": [
    "# 1. Read in the AWS S3 bucket into a DataFrame.\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoljcJ7WPpnm"
   },
   "outputs": [],
   "source": [
    "# 2. Create a temporary view of the DataFrame.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "\n",
    "# Print Spark version to verify installation\n",
    "print(\"Spark version:\", spark.version)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CreateTempView\").getOrCreate()\n",
    "\n",
    "# URL of the CSV file\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
    "\n",
    "# Add file to Spark context\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "# Load CSV file into a DataFrame\n",
    "df = spark.read.csv(\"file://\" + SparkFiles.get(\"home_sales_revised.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "# Create a temporary view\n",
    "df.createOrReplaceTempView(\"home_sales\")\n",
    "\n",
    "# Example SQL query on the temporary view\n",
    "result = spark.sql(\"SELECT * FROM home_sales LIMIT 5\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6fkwOeOmqvq"
   },
   "outputs": [],
   "source": [
    "# 3. What is the average price for a four bedroom house sold per year, rounded to two decimal places?\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, round\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"AveragePriceFourBedroom\").getOrCreate()\n",
    "\n",
    "# URL of the CSV file\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
    "\n",
    "# Add file to Spark context\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "# Load CSV file into a DataFrame\n",
    "df = spark.read.csv(\"file://\" + SparkFiles.get(\"home_sales_revised.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "# Filter data for four-bedroom houses\n",
    "four_bedroom_sales = df.filter(col(\"BEDS\") == 4)\n",
    "\n",
    "# Calculate average price per year\n",
    "average_price_per_year = four_bedroom_sales.groupBy(\"Yr_Sold\") \\\n",
    "                                           .agg(round(avg(\"PRICE\"), 2).alias(\"Average_Price\")) \\\n",
    "                                           .orderBy(\"Yr_Sold\")\n",
    "\n",
    "# Show the result\n",
    "average_price_per_year.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8p_tUS8h8it"
   },
   "outputs": [],
   "source": [
    "# 4. from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, round\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"AveragePriceThreeBedThreeBath\").getOrCreate()\n",
    "\n",
    "# URL of the CSV file\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
    "\n",
    "# Add file to Spark context\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "# Load CSV file into a DataFrame\n",
    "df = spark.read.csv(\"file://\" + SparkFiles.get(\"home_sales_revised.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "# Filter data for homes with 3 bedrooms and 3 bathrooms\n",
    "three_bed_three_bath_sales = df.filter((col(\"BEDS\") == 3) & (col(\"BATHS\") == 3))\n",
    "\n",
    "# Calculate average price per year built\n",
    "average_price_per_year_built = three_bed_three_bath_sales.groupBy(\"Yr_Built\") \\\n",
    "                                                         .agg(round(avg(\"PRICE\"), 2).alias(\"Average_Price\")) \\\n",
    "                                                         .orderBy(\"Yr_Built\")\n",
    "\n",
    "# Show the result\n",
    "average_price_per_year_built.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-Eytz64liDU"
   },
   "outputs": [],
   "source": [
    "# 5. What is the average price of a home for each year the home was built,\n",
    "# that have 3 bedrooms, 3 bathrooms, with two floors,\n",
    "# and are greater than or equal to 2,000 square feet, rounded to two decimal places?\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, round\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"AveragePriceFilteredHomes\").getOrCreate()\n",
    "\n",
    "# URL of the CSV file\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
    "\n",
    "# Add file to Spark context\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "# Load CSV file into a DataFrame\n",
    "df = spark.read.csv(\"file://\" + SparkFiles.get(\"home_sales_revised.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "# Filter data for homes with 3 bedrooms, 3 bathrooms, two floors, and >= 2000 sqft\n",
    "filtered_sales = df.filter((col(\"BEDS\") == 3) &\n",
    "                           (col(\"BATHS\") == 3) &\n",
    "                           (col(\"FLOORS\") == 2) &\n",
    "                           (col(\"SQUARE_FEET\") >= 2000))\n",
    "\n",
    "# Calculate average price per year built\n",
    "average_price_per_year_built = filtered_sales.groupBy(\"Yr_Built\") \\\n",
    "                                             .agg(round(avg(\"PRICE\"), 2).alias(\"Average_Price\")) \\\n",
    "                                             .orderBy(\"Yr_Built\")\n",
    "\n",
    "# Show the result\n",
    "average_price_per_year_built.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUrfgOX1pCRd",
    "outputId": "0a645986-7179-440c-c0bc-345f1693f8af"
   },
   "outputs": [],
   "source": [
    "# 6. What is the average price of a home per \"view\" rating, rounded to two decimal places,\n",
    "# having an average home price greater than or equal to $350,000? Order by descending view rating. \n",
    "# Although this is a small dataset, determine the run time for this query.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAhk3ZD2tFy8"
   },
   "outputs": [],
   "source": [
    "# 7. Cache the the temporary table home_sales.\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CacheTempTable\").getOrCreate()\n",
    "\n",
    "# URL of the CSV file\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
    "\n",
    "# Add file to Spark context\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "# Load CSV file into a DataFrame\n",
    "df = spark.read.csv(\"file://\" + SparkFiles.get(\"home_sales_revised.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "# Create a temporary view (or table) from DataFrame\n",
    "df.createOrReplaceTempView(\"home_sales\")\n",
    "\n",
    "# Cache the temporary table\n",
    "spark.sql(\"CACHE TABLE home_sales\")\n",
    "\n",
    "# Example: Query the cached table\n",
    "result = spark.sql(\"SELECT COUNT(*) FROM home_sales\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4opVhbvxtL-i"
   },
   "outputs": [],
   "source": [
    "# 8. Check if the table is cached.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CheckCache\").getOrCreate()\n",
    "\n",
    "# Check if the table is cached\n",
    "is_cached = spark.catalog.isCached(\"home_sales\")\n",
    "\n",
    "if is_cached:\n",
    "    print(\"The table 'home_sales' is cached.\")\n",
    "else:\n",
    "    print(\"The table 'home_sales' is not cached.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5GnL46lwTSEk",
    "outputId": "63c7dc50-d96a-4a48-97b6-91a446cdb973"
   },
   "outputs": [],
   "source": [
    "# 9. Using the cached data, run the last query above, that calculates \n",
    "# the average price of a home per \"view\" rating, rounded to two decimal places,\n",
    "# having an average home price greater than or equal to $350,000. \n",
    "# Determine the runtime and compare it to the uncached runtime.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qm12WN9isHBR"
   },
   "outputs": [],
   "source": [
    "# 10. Partition by the \"date_built\" field on the formatted parquet home sales data \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PartitionByDateBuilt\").getOrCreate()\n",
    "\n",
    "# Load formatted Parquet data\n",
    "parquet_path = \"path_to_your_formatted_parquet_data\"  # Replace with your actual Parquet file path\n",
    "df = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Partition by the 'date_built' field\n",
    "partitioned_df = df.withColumn(\"date_built\", col(\"date_built\").cast(\"date\")) \\\n",
    "                   .write.partitionBy(\"date_built\") \\\n",
    "                   .mode(\"overwrite\") \\\n",
    "                   .parquet(\"partitioned_parquet_path\")\n",
    "\n",
    "# Replace 'partitioned_parquet_path' with your desired output directory where partitioned Parquet files will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZ7BgY61sRqY"
   },
   "outputs": [],
   "source": [
    "# 11. Read the parquet formatted data.from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ReadPartitionedParquet\").getOrCreate()\n",
    "\n",
    "# Path to the directory containing partitioned Parquet files\n",
    "partitioned_parquet_path = \"partitioned_parquet_path\"  # Replace with your actual partitioned Parquet directory\n",
    "\n",
    "# Read partitioned Parquet data\n",
    "partitioned_df = spark.read.parquet(partitioned_parquet_path)\n",
    "\n",
    "# Show the schema and some sample data\n",
    "partitioned_df.printSchema()\n",
    "partitioned_df.show(5, truncate=False)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ReadPartitionedParquet\").getOrCreate()\n",
    "\n",
    "# Path to the directory containing partitioned Parquet files\n",
    "partitioned_parquet_path = \"partitioned_parquet_path\"  # Replace with your actual partitioned Parquet directory\n",
    "\n",
    "# Read partitioned Parquet data\n",
    "partitioned_df = spark.read.parquet(partitioned_parquet_path)\n",
    "\n",
    "# Show the schema and some sample data\n",
    "partitioned_df.printSchema()\n",
    "partitioned_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6MJkHfvVcvh"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CreateTempTableFromParquet\").getOrCreate()\n",
    "\n",
    "# Path to the directory containing partitioned Parquet files\n",
    "partitioned_parquet_path = \"partitioned_parquet_path\"  # Replace with your actual partitioned Parquet directory\n",
    "\n",
    "# Read partitioned Parquet data\n",
    "partitioned_df = spark.read.parquet(partitioned_parquet_path)\n",
    "\n",
    "# Create or replace a temporary view for the DataFrame\n",
    "partitioned_df.createOrReplaceTempView(\"temp_partitioned_table\")\n",
    "\n",
    "# Example query on the temporary table\n",
    "query_result = spark.sql(\"SELECT * FROM temp_partitioned_table LIMIT 5\")\n",
    "\n",
    "# Show the query result\n",
    "query_result.show()# 12. Create a temporary table for the parquet data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_Vhb52rU1Sn",
    "outputId": "d6748ea6-d70a-41fd-dcb8-c214a85e949e"
   },
   "outputs": [],
   "source": [
    "# 13. Using the parquet DataFrame, run the last query above, that calculates \n",
    "# the average price of a home per \"view\" rating, rounded to two decimal places,\n",
    "# having an average home price greater than or equal to $350,000. \n",
    "# Determine the runtime and compare it to the cached runtime.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"AveragePriceByViewRating\").getOrCreate()\n",
    "\n",
    "# Path to the directory containing partitioned Parquet files\n",
    "partitioned_parquet_path = \"partitioned_parquet_path\"  # Replace with your actual partitioned Parquet directory\n",
    "\n",
    "# Read partitioned Parquet data\n",
    "parquet_df = spark.read.parquet(partitioned_parquet_path)\n",
    "\n",
    "# Create or replace a temporary view for the DataFrame\n",
    "parquet_df.createOrReplaceTempView(\"temp_partitioned_table\")\n",
    "\n",
    "# Start measuring time\n",
    "start_time = time.time()\n",
    "\n",
    "# Query to calculate average price per \"view\" rating and filter by average price >= $350,000\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        view_rating,\n",
    "        ROUND(AVG(price), 2) AS avg_price\n",
    "    FROM \n",
    "        temp_partitioned_table\n",
    "    GROUP BY \n",
    "        view_rating\n",
    "    HAVING \n",
    "        AVG(price) >= 350000\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result = spark.sql(query)\n",
    "\n",
    "# Show query result\n",
    "result.show()\n",
    "\n",
    "# Calculate and print runtime\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Optionally, cache the DataFrame and re-run the query to compare runtime\n",
    "parquet_df.cache()\n",
    "\n",
    "# Start measuring time for cached query\n",
    "start_time_cached = time.time()\n",
    "\n",
    "# Execute the query again after caching\n",
    "result_cached = spark.sql(query)\n",
    "\n",
    "# Show cached query result\n",
    "result_cached.show()\n",
    "\n",
    "# Calculate and print cached runtime\n",
    "print(\"--- Cached Runtime: %s seconds ---\" % (time.time() - start_time_cached))\n",
    "\n",
    "# Unpersist the DataFrame from memory\n",
    "parquet_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjjYzQGjtbq8"
   },
   "outputs": [],
   "source": [
    "# 14. Uncache the home_sales temporary table.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"UncacheTempTable\").getOrCreate()\n",
    "\n",
    "# Uncache the temporary table 'home_sales'\n",
    "spark.catalog.uncacheTable(\"home_sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sy9NBvO7tlmm"
   },
   "outputs": [],
   "source": [
    "# 15. Check if the home_sales is no longer cached\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CheckCacheStatus\").getOrCreate()\n",
    "\n",
    "# Check if 'home_sales' table is cached\n",
    "is_cached = spark.catalog.isCached(\"home_sales\")\n",
    "\n",
    "if is_cached:\n",
    "    print(\"The table 'home_sales' is still cached.\")\n",
    "else:\n",
    "    print(\"The table 'home_sales' is not cached anymore.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
